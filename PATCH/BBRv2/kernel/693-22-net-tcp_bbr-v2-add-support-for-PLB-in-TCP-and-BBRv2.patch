From cf9b1dacabb1ef62481a452f7f169e1679e2da49 Mon Sep 17 00:00:00 2001
From: Mubashir Adnan Qureshi <mubashirq@google.com>
Date: Wed, 20 Jul 2022 00:11:26 +0000
Subject: [PATCH] net-tcp_bbr: v2: add support for PLB in TCP and BBRv2

PLB (Protective Load Balancing) is a host based mechanism for load
balancing across switch links. It leverages congestion signals(e.g. ECN)
from transport layer to randomly change the path of the connection
experiencing congestion. PLB changes the path of the connection by
changing the outgoing IPv6 flow label for IPv6 connections (implemented
in Linux by calling sk_rethink_txhash()). Because of this implementation
mechanism, PLB can currently only work for IPv6 traffic. For more
information, see the SIGCOMM 2022 paper:
  https://doi.org/10.1145/3544216.3544226

Congestion control algorithms track PLB state and cause the connection
to trigger a path change when either of the 2 conditions is satisfied:

- No packets are in flight and (# consecutive congested rounds >=
  sysctl_tcp_plb_idle_rehash_rounds)
- (# consecutive congested rounds >= sysctl_tcp_plb_rehash_rounds)

A round (RTT) is marked as congested when congestion signal
(ECN ce_ratio) over an RTT is greater than sysctl_tcp_plb_cong_thresh.
In the event of RTO, PLB (via tcp_write_timeout()) triggers a path
change and disables congestion-triggered path changes for random time
between (sysctl_tcp_plb_suspend_rto_sec, 2*sysctl_tcp_plb_suspend_rto_sec)
to avoid hopping onto the "connectivity blackhole". RTO-triggered
path changes can still happen during this cool-off period.

Change-Id: I5d0fb3ab55b27b506b0cf32bc93df892b5336c2c
---
 Documentation/networking/ip-sysctl.rst |  58 ++++++++++++++
 include/linux/tcp.h                    |   3 +
 include/net/inet_connection_sock.h     |   2 +-
 include/net/netns/ipv4.h               |   5 ++
 include/net/tcp.h                      |  17 +++++
 include/uapi/linux/snmp.h              |   1 +
 net/ipv4/Makefile                      |   2 +-
 net/ipv4/proc.c                        |   1 +
 net/ipv4/sysctl_net_ipv4.c             |  43 +++++++++++
 net/ipv4/tcp_bbr2.c                    |  28 +++++--
 net/ipv4/tcp_ipv4.c                    |   7 ++
 net/ipv4/tcp_plb.c                     | 100 +++++++++++++++++++++++++
 12 files changed, 260 insertions(+), 7 deletions(-)
 create mode 100644 net/ipv4/tcp_plb.c

--- a/Documentation/networking/ip-sysctl.rst
+++ b/Documentation/networking/ip-sysctl.rst
@@ -922,6 +922,64 @@ tcp_rx_skb_cache - BOOLEAN
 
 	Default: 0 (disabled)
 
+tcp_plb_enabled - BOOLEAN
+	If set, TCP PLB (Protective Load Balancing) is enabled. PLB is
+	described in the following paper:
+	https://doi.org/10.1145/3544216.3544226. Based on PLB parameters,
+	upon sensing sustained congestion, TCP triggers a change in
+	flow label field for outgoing IPv6 packets. A change in flow label
+	field potentially changes the path of outgoing packets for switches
+	that use ECMP/WCMP for routing.
+
+	Default: 0
+
+tcp_plb_cong_thresh - INTEGER
+	Fraction of packets marked with congestion over a round (RTT) to
+	tag that round as congested. This is referred to as K in the PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	The 0-1 fraction range is mapped to 0-256 range to avoid floating
+	point operations. For example, 128 means that if at least 50% of
+	the packets in a round were marked as congested then the round
+	will be tagged as congested.
+
+	Possible Values: 0 - 256
+
+	Default: 128
+
+tcp_plb_idle_rehash_rounds - INTEGER
+	Number of consecutive congested rounds (RTT) seen after which
+	a rehash can be performed, given there are no packets in flight.
+	This is referred to as M in PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	Possible Values: 0 - 31
+
+	Default: 3
+
+tcp_plb_rehash_rounds - INTEGER
+	Number of consecutive congested rounds (RTT) seen after which
+	a forced rehash can be performed. Be careful when setting this
+	parameter, as a small value increases the risk of retransmissions.
+	This is referred to as N in PLB paper:
+	https://doi.org/10.1145/3544216.3544226.
+
+	Possible Values: 0 - 31
+
+	Default: 12
+
+tcp_plb_suspend_rto_sec - INTEGER
+	Time, in seconds, to suspend PLB in event of an RTO. In order to avoid
+	having PLB repath onto a connectivity "black hole", after an RTO a TCP
+	connection suspends PLB repathing for a random duration between 1x and
+	2x of this parameter. Randomness is added to avoid concurrent rehashing
+	of multiple TCP connections. This should be set corresponding to the
+	amount of time it takes to repair a failed link.
+
+	Possible Values: 0 - 255
+
+	Default: 60
+
 UDP variables
 =============
 
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -413,6 +413,9 @@ struct tcp_sock {
 	 */
 	struct request_sock __rcu *fastopen_rsk;
 	struct saved_syn *saved_syn;
+
+/* Rerouting information */
+	u16	ecn_rehash;	/* PLB triggered rehash attempts */
 };
 
 enum tsq_enum {
--- a/include/net/inet_connection_sock.h
+++ b/include/net/inet_connection_sock.h
@@ -135,7 +135,7 @@ struct inet_connection_sock {
 	u32			  icsk_user_timeout;
 
 /* XXX inflated by temporary internal debugging info */
-#define ICSK_CA_PRIV_SIZE      (216)
+#define ICSK_CA_PRIV_SIZE      (224)
 	u64			  icsk_ca_priv[ICSK_CA_PRIV_SIZE / sizeof(u64)];
 };
 
--- a/include/net/netns/ipv4.h
+++ b/include/net/netns/ipv4.h
@@ -184,6 +184,11 @@ struct netns_ipv4 {
 	atomic_t tfo_active_disable_times;
 	unsigned long tfo_active_disable_stamp;
 	int sysctl_tcp_reflect_tos;
+	u8 sysctl_tcp_plb_enabled;
+	int sysctl_tcp_plb_cong_thresh;
+	u8 sysctl_tcp_plb_idle_rehash_rounds;
+	u8 sysctl_tcp_plb_rehash_rounds;
+	u8 sysctl_tcp_plb_suspend_rto_sec;
 
 	int sysctl_udp_wmem_min;
 	int sysctl_udp_rmem_min;
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -2115,6 +2115,23 @@ extern void tcp_rack_advance(struct tcp_
 extern void tcp_rack_reo_timeout(struct sock *sk);
 extern void tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs);
 
+/* tcp_plb.c */
+
+#define TCP_PLB_SCALE 8	/* scaling factor for fractions in PLB (e.g. ce_ratio) */
+
+/* State for PLB (Protective Load Balancing) for a single TCP connection. */
+struct tcp_plb_state {
+	u8	consec_cong_rounds:5, /* consecutive congested rounds */
+		enabled:1,	/* Check if PLB is enabled */
+		unused:2;
+	u32	pause_until; /* jiffies32 when PLB can resume repathing */
+};
+
+void tcp_plb_update_state(const struct sock *sk, struct tcp_plb_state *plb,
+			  const int cong_ratio);
+void tcp_plb_check_rehash(struct sock *sk, struct tcp_plb_state *plb);
+void tcp_plb_update_state_upon_rto(struct sock *sk, struct tcp_plb_state *plb);
+
 /* At how many usecs into the future should the RTO fire? */
 static inline s64 tcp_rto_delta_us(const struct sock *sk)
 {
--- a/include/uapi/linux/snmp.h
+++ b/include/uapi/linux/snmp.h
@@ -289,6 +289,7 @@ enum
 	LINUX_MIB_TCPDUPLICATEDATAREHASH,	/* TCPDuplicateDataRehash */
 	LINUX_MIB_TCPDSACKRECVSEGS,		/* TCPDSACKRecvSegs */
 	LINUX_MIB_TCPDSACKIGNOREDDUBIOUS,	/* TCPDSACKIgnoredDubious */
+	LINUX_MIB_TCPECNREHASH,			/* TCPECNRehash */
 	__LINUX_MIB_MAX
 };
 
--- a/net/ipv4/Makefile
+++ b/net/ipv4/Makefile
@@ -10,7 +10,7 @@ obj-y     := route.o inetpeer.o protocol
 	     tcp.o tcp_input.o tcp_output.o tcp_timer.o tcp_ipv4.o \
 	     tcp_minisocks.o tcp_cong.o tcp_metrics.o tcp_fastopen.o \
 	     tcp_rate.o tcp_recovery.o tcp_ulp.o \
-	     tcp_offload.o datagram.o raw.o udp.o udplite.o \
+	     tcp_offload.o tcp_plb.o datagram.o raw.o udp.o udplite.o \
 	     udp_offload.o arp.o icmp.o devinet.o af_inet.o igmp.o \
 	     fib_frontend.o fib_semantics.o fib_trie.o fib_notifier.o \
 	     inet_fragment.o ping.o ip_tunnel_core.o gre_offload.o \
--- a/net/ipv4/proc.c
+++ b/net/ipv4/proc.c
@@ -294,6 +294,7 @@ static const struct snmp_mib snmp4_net_l
 	SNMP_MIB_ITEM("TcpDuplicateDataRehash", LINUX_MIB_TCPDUPLICATEDATAREHASH),
 	SNMP_MIB_ITEM("TCPDSACKRecvSegs", LINUX_MIB_TCPDSACKRECVSEGS),
 	SNMP_MIB_ITEM("TCPDSACKIgnoredDubious", LINUX_MIB_TCPDSACKIGNOREDDUBIOUS),
+	SNMP_MIB_ITEM("TCPECNRehash", LINUX_MIB_TCPECNREHASH),
 	SNMP_MIB_SENTINEL
 };
 
--- a/net/ipv4/sysctl_net_ipv4.c
+++ b/net/ipv4/sysctl_net_ipv4.c
@@ -50,6 +50,8 @@ static int ip_ping_group_range_max[] = {
 static int comp_sack_nr_max = 255;
 static u32 u32_max_div_HZ = UINT_MAX / HZ;
 static int one_day_secs = 24 * 3600;
+static int tcp_plb_max_rounds = 31;
+static int tcp_plb_max_cong_thresh = 256;
 
 /* obsolete */
 static int sysctl_tcp_low_latency __read_mostly;
@@ -1354,6 +1356,47 @@ static struct ctl_table ipv4_net_table[]
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= SYSCTL_ONE
 	},
+	{
+		.procname       = "tcp_plb_enabled",
+		.data           = &init_net.ipv4.sysctl_tcp_plb_enabled,
+		.maxlen         = sizeof(u8),
+		.mode           = 0644,
+		.proc_handler   = proc_dou8vec_minmax,
+		.extra1         = SYSCTL_ZERO,
+		.extra2         = SYSCTL_ONE,
+	},
+	{
+		.procname       = "tcp_plb_cong_thresh",
+		.data           = &init_net.ipv4.sysctl_tcp_plb_cong_thresh,
+		.maxlen         = sizeof(int),
+		.mode           = 0644,
+		.proc_handler   = proc_dointvec_minmax,
+		.extra1         = SYSCTL_ZERO,
+		.extra2         = &tcp_plb_max_cong_thresh,
+	},
+	{
+		.procname       = "tcp_plb_idle_rehash_rounds",
+		.data           = &init_net.ipv4.sysctl_tcp_plb_idle_rehash_rounds,
+		.maxlen         = sizeof(u8),
+		.mode           = 0644,
+		.proc_handler   = proc_dou8vec_minmax,
+		.extra2		= &tcp_plb_max_rounds,
+	},
+	{
+		.procname       = "tcp_plb_rehash_rounds",
+		.data           = &init_net.ipv4.sysctl_tcp_plb_rehash_rounds,
+		.maxlen         = sizeof(u8),
+		.mode           = 0644,
+		.proc_handler   = proc_dou8vec_minmax,
+		.extra2         = &tcp_plb_max_rounds,
+	},
+	{
+		.procname       = "tcp_plb_suspend_rto_sec",
+		.data           = &init_net.ipv4.sysctl_tcp_plb_suspend_rto_sec,
+		.maxlen         = sizeof(u8),
+		.mode           = 0644,
+		.proc_handler   = proc_dou8vec_minmax,
+	},
 	{ }
 };
 
--- a/net/ipv4/tcp_bbr2.c
+++ b/net/ipv4/tcp_bbr2.c
@@ -167,6 +167,7 @@ struct bbr {
 		initialized:1;	       /* has bbr_init() been called? */
 	u32	alpha_last_delivered;	 /* tp->delivered    at alpha update */
 	u32	alpha_last_delivered_ce; /* tp->delivered_ce at alpha update */
+	struct	tcp_plb_state plb;
 
 	/* Params configurable using setsockopt. Refer to correspoding
 	 * module param for detailed description of params.
@@ -733,7 +734,11 @@ static void bbr_cwnd_event(struct sock *
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
 
-	if (event == CA_EVENT_TX_START && tp->app_limited) {
+	if (event == CA_EVENT_TX_START) {
+		tcp_plb_check_rehash(sk, &bbr->plb);
+
+		if (!tp->app_limited)
+			return;
 		bbr->idle_restart = 1;
 		bbr->ack_epoch_mstamp = tp->tcp_mstamp;
 		bbr->ack_epoch_acked = 0;
@@ -1389,7 +1394,7 @@ static void bbr2_check_ecn_too_high_in_s
 	}
 }
 
-static void bbr2_update_ecn_alpha(struct sock *sk)
+static int bbr2_update_ecn_alpha(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
@@ -1398,14 +1403,14 @@ static void bbr2_update_ecn_alpha(struct
 	u32 gain;
 
 	if (bbr->params.ecn_factor == 0)
-		return;
+		return -1;
 
 	delivered = tp->delivered - bbr->alpha_last_delivered;
 	delivered_ce = tp->delivered_ce - bbr->alpha_last_delivered_ce;
 
 	if (delivered == 0 ||		/* avoid divide by zero */
 	    WARN_ON_ONCE(delivered < 0 || delivered_ce < 0))  /* backwards? */
-		return;
+		return -1;
 
 	/* See if we should use ECN sender logic for this connection. */
 	if (!bbr->ecn_eligible && bbr_ecn_enable &&
@@ -1424,6 +1429,7 @@ static void bbr2_update_ecn_alpha(struct
 	bbr->alpha_last_delivered_ce = tp->delivered_ce;
 
 	bbr2_check_ecn_too_high_in_startup(sk, ce_ratio);
+	return (int)ce_ratio;
 }
 
 /* Each round trip of BBR_BW_PROBE_UP, double volume of probing data. */
@@ -2238,6 +2244,7 @@ static void bbr2_main(struct sock *sk, c
 	struct bbr_context ctx = { 0 };
 	bool update_model = true;
 	u32 bw;
+	int ce_ratio = -1;
 
 	bbr->debug.event = '.';  /* init to default NOP (no event yet) */
 
@@ -2245,7 +2252,9 @@ static void bbr2_main(struct sock *sk, c
 	if (bbr->round_start) {
 		bbr->rounds_since_probe =
 			min_t(s32, bbr->rounds_since_probe + 1, 0xFF);
-		bbr2_update_ecn_alpha(sk);
+		ce_ratio = bbr2_update_ecn_alpha(sk);
+		tcp_plb_update_state(sk, &bbr->plb, ce_ratio);
+		tcp_plb_check_rehash(sk, &bbr->plb);
 	}
 
 	bbr->ecn_in_round  |= rs->is_ece;
@@ -2408,6 +2417,7 @@ static void bbr2_init(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct bbr *bbr = inet_csk_ca(sk);
+	const struct net *net = sock_net(sk);
 
 	bbr_init(sk);	/* run shared init code for v1 and v2 */
 
@@ -2470,6 +2480,13 @@ static void bbr2_init(struct sock *sk)
 	bbr->alpha_last_delivered = 0;
 	bbr->alpha_last_delivered_ce = 0;
 
+	bbr->plb.enabled = 0;
+	bbr->plb.consec_cong_rounds = 0;
+	bbr->plb.pause_until = 0;
+	if ((tp->ecn_flags & TCP_ECN_OK) &&
+	    net->ipv4.sysctl_tcp_plb_enabled)
+		bbr->plb.enabled = 1;
+
 	tp->fast_ack_mode = min_t(u32, 0x2U, bbr_fast_ack_mode);
 
 	if ((tp->ecn_flags & TCP_ECN_OK) && bbr_ecn_enable)
@@ -2614,6 +2631,7 @@ static void bbr2_set_state(struct sock *
 		struct rate_sample rs = { .losses = 1 };
 		struct bbr_context ctx = { 0 };
 
+		tcp_plb_update_state_upon_rto(sk, &bbr->plb);
 		bbr->prev_ca_state = TCP_CA_Loss;
 		bbr->full_bw = 0;
 		if (!bbr2_is_probing_bandwidth(sk) && bbr->inflight_lo == ~0U) {
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -2932,6 +2932,13 @@ static int __net_init tcp_sk_init(struct
 	net->ipv4.sysctl_tcp_fastopen_blackhole_timeout = 0;
 	atomic_set(&net->ipv4.tfo_active_disable_times, 0);
 
+	/* Set default values for PLB */
+	net->ipv4.sysctl_tcp_plb_enabled = 0; /* Disabled by default */
+	net->ipv4.sysctl_tcp_plb_cong_thresh = 128; /* 50% congestion */
+	net->ipv4.sysctl_tcp_plb_idle_rehash_rounds = 3;
+	net->ipv4.sysctl_tcp_plb_rehash_rounds = 12;
+	net->ipv4.sysctl_tcp_plb_suspend_rto_sec = 60;
+
 	/* Reno is always built in */
 	if (!net_eq(net, &init_net) &&
 	    bpf_try_module_get(init_net.ipv4.tcp_congestion_control,
--- /dev/null
+++ b/net/ipv4/tcp_plb.c
@@ -0,0 +1,100 @@
+/* Protective Load Balancing (PLB)
+ *
+ * PLB was designed to reduce link load imbalance across datacenter
+ * switches. PLB is a host-based optimization; it leverages congestion
+ * signals from the transport layer to randomly change the path of the
+ * connection experiencing sustained congestion. PLB prefers to repath
+ * after idle periods to minimize packet reordering. It repaths by
+ * changing the IPv6 Flow Label on the packets of a connection, which
+ * datacenter switches include as part of ECMP/WCMP hashing.
+ *
+ * PLB is described in detail in:
+ *
+ *	Mubashir Adnan Qureshi, Yuchung Cheng, Qianwen Yin, Qiaobin Fu,
+ *	Gautam Kumar, Masoud Moshref, Junhua Yan, Van Jacobson,
+ *	David Wetherall,Abdul Kabbani:
+ *	"PLB: Congestion Signals are Simple and Effective for
+ *	 Network Load Balancing"
+ *	In ACM SIGCOMM 2022, Amsterdam Netherlands.
+ *
+ */
+
+#include <net/tcp.h>
+
+/* Called once per round-trip to update PLB state for a connection. */
+void tcp_plb_update_state(const struct sock *sk, struct tcp_plb_state *plb,
+			  const int cong_ratio)
+{
+	struct net *net = sock_net(sk);
+
+	if (!plb->enabled)
+		return;
+
+	if (cong_ratio >= 0) {
+		if (cong_ratio < net->ipv4.sysctl_tcp_plb_cong_thresh)
+			plb->consec_cong_rounds = 0;
+		else if (plb->consec_cong_rounds <
+			 net->ipv4.sysctl_tcp_plb_rehash_rounds)
+			plb->consec_cong_rounds++;
+	}
+}
+EXPORT_SYMBOL_GPL(tcp_plb_update_state);
+
+/* Check whether recent congestion has been persistent enough to warrant
+ * a load balancing decision that switches the connection to another path.
+ */
+void tcp_plb_check_rehash(struct sock *sk, struct tcp_plb_state *plb)
+{
+	struct net *net = sock_net(sk);
+	bool can_idle_rehash, can_force_rehash;
+
+	if (!plb->enabled)
+		return;
+
+	/* Note that tcp_jiffies32 can wrap, so we clear pause_until
+	 * to 0 to indicate there is no recent RTO event that constrains
+	 * PLB rehashing.
+	 */
+	if (plb->pause_until &&
+	    !before(tcp_jiffies32, plb->pause_until))
+		plb->pause_until = 0;
+
+	can_idle_rehash = net->ipv4.sysctl_tcp_plb_idle_rehash_rounds &&
+			  !tcp_sk(sk)->packets_out &&
+			  plb->consec_cong_rounds >=
+			  net->ipv4.sysctl_tcp_plb_idle_rehash_rounds;
+	can_force_rehash = plb->consec_cong_rounds >=
+			   net->ipv4.sysctl_tcp_plb_rehash_rounds;
+
+	if (!plb->pause_until && (can_idle_rehash || can_force_rehash)) {
+		sk_rethink_txhash(sk);
+		plb->consec_cong_rounds = 0;
+		tcp_sk(sk)->ecn_rehash++;
+		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPECNREHASH);
+	}
+}
+EXPORT_SYMBOL_GPL(tcp_plb_check_rehash);
+
+/* Upon RTO, disallow load balancing for a while, to avoid having load
+ * balancing decisions switch traffic to a black-holed path that was
+ * previously avoided with a sk_rethink_txhash() call at RTO time.
+ */
+void tcp_plb_update_state_upon_rto(struct sock *sk, struct tcp_plb_state *plb)
+{
+	struct net *net = sock_net(sk);
+	u32 pause;
+
+	if (!plb->enabled)
+		return;
+
+	pause = net->ipv4.sysctl_tcp_plb_suspend_rto_sec * HZ;
+	pause += prandom_u32_max(pause);
+	plb->pause_until = tcp_jiffies32 + pause;
+
+	/* Reset PLB state upon RTO, since an RTO causes a sk_rethink_txhash() call
+	 * that may switch this connection to a path with completely different
+	 * congestion characteristics.
+	 */
+	plb->consec_cong_rounds = 0;
+}
+EXPORT_SYMBOL_GPL(tcp_plb_update_state_upon_rto);
